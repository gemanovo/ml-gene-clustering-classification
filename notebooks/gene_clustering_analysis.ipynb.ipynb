{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee6c02b2",
      "metadata": {
        "id": "ee6c02b2"
      },
      "source": [
        "<a id=\"0\"></a> <br>\n",
        "# Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c00e4d",
      "metadata": {
        "id": "43c00e4d"
      },
      "source": [
        "1. [Load the data](#7)\n",
        "1. [Preprocessing the data](#2)\n",
        "1. [Cluster Analysis](#6)\n",
        "    1. [Hierarchical Clustering](#8)\n",
        "    1. [Partitional Clustering](#9)\n",
        "    1. [DBSCAN](#10)\n",
        "    1. [Gaussian Mixture Model](#11)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load the data"
      ],
      "metadata": {
        "id": "fVNLTtHGAkSA"
      },
      "id": "fVNLTtHGAkSA"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('data.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ZFkqSWYj5DFy"
      },
      "id": "ZFkqSWYj5DFy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "-004No5XPxCx"
      },
      "id": "-004No5XPxCx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_statistics = data.describe()\n",
        "summary_statistics"
      ],
      "metadata": {
        "id": "afV_Q2DLP1lo"
      },
      "id": "afV_Q2DLP1lo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = data.columns\n",
        "column_names"
      ],
      "metadata": {
        "id": "IYw9kiKrQqRv"
      },
      "id": "IYw9kiKrQqRv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of duplicate rows: {data.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "BP3cRR7iQK-n"
      },
      "id": "BP3cRR7iQK-n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum().value_counts())"
      ],
      "metadata": {
        "id": "T_m1-BIIQv_I"
      },
      "id": "T_m1-BIIQv_I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the categorical columns in the data, to know what we need to convert\n",
        "categorical_data = data.select_dtypes(include=['object'])\n",
        "print(categorical_data)"
      ],
      "metadata": {
        "id": "m9qPiEWbTIXX"
      },
      "id": "m9qPiEWbTIXX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count unique values for each specified object-type column and display the counts\n",
        "object_columns = data[[\"U95019_s_at\",\"HG3729-HT3999_f_at\"]]\n",
        "for column in object_columns.columns:\n",
        "    value_counts = object_columns[column].value_counts()\n",
        "    print(f\"\\nValue counts for '{column}':\")\n",
        "    print(value_counts)"
      ],
      "metadata": {
        "id": "e9snrEtqTrPc"
      },
      "id": "e9snrEtqTrPc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(data=data, x='U95019_s_at')\n",
        "plt.show()\n",
        "sns.countplot(data=data, x='HG3729-HT3999_f_at')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PYppc6fLJ6WG"
      },
      "id": "PYppc6fLJ6WG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "numeric_data = data.select_dtypes(include=np.number)\n",
        "means = numeric_data.mean(axis=0)\n",
        "stds = numeric_data.std(axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(means, kde=True)\n",
        "plt.title(\"Distribution of Means\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(stds, kde=True)\n",
        "plt.title(\"Distribution of Standard Deviations\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Br21F4jyf3kg"
      },
      "id": "Br21F4jyf3kg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing the data"
      ],
      "metadata": {
        "id": "XE43Aw5vhJnb"
      },
      "id": "XE43Aw5vhJnb"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "categorical_columns = ['U95019_s_at', 'HG3729-HT3999_f_at']\n",
        "numerical_columns = list(data.drop(categorical_columns, axis=1).columns)\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42) #Split the data before fitting the pipeline\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('ordinal', OrdinalEncoder(), categorical_columns),\n",
        "        ('scaler', RobustScaler(quantile_range=(25.0, 75.0)), numerical_columns)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "train_processed = pipeline.fit_transform(train_data)\n",
        "test_processed = pipeline.transform(test_data)\n",
        "\n",
        "print(\"Train shape:\", train_processed.shape)\n",
        "print(\"Test shape:\", test_processed.shape)\n"
      ],
      "metadata": {
        "id": "d2MST5vb6U_B"
      },
      "id": "d2MST5vb6U_B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_processed = pipeline.fit_transform(train_data)\n",
        "\n",
        "ordinal_encoded_data = pipeline.named_steps['preprocessor'].transformers_[0][1].transform(train_data[['U95019_s_at', 'HG3729-HT3999_f_at']])\n",
        "ordinal_encoded_df = pd.DataFrame(ordinal_encoded_data, columns=['U95019_s_at', 'HG3729-HT3999_f_at'], index=train_data.index)\n",
        "\n",
        "print(ordinal_encoded_df)\n"
      ],
      "metadata": {
        "id": "6Qn703HhsOdW"
      },
      "id": "6Qn703HhsOdW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "means = train_processed.mean(axis = 0)\n",
        "stds = train_processed.std(axis = 0)\n",
        "\n",
        "plt.figure(figsize = (12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(means, kde = True)\n",
        "plt.title(\"Distribution of Means\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(stds, kde = True)\n",
        "plt.title(\"Distribution of Standard Deviations\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eVL3qI_Aolqa"
      },
      "id": "eVL3qI_Aolqa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c6c9190b",
      "metadata": {
        "id": "c6c9190b"
      },
      "source": [
        "# 3. Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NPp2j96r1rb2",
      "metadata": {
        "id": "NPp2j96r1rb2"
      },
      "source": [
        "<a id=\"8\"></a>\n",
        "## Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sch\n",
        "from scipy.cluster.hierarchy import cophenet\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "methods = ['single', 'complete', 'average', 'ward']\n",
        "distance_metrics = ['euclidean', 'cityblock']\n",
        "\n",
        "best_coefficient = -1\n",
        "best_combination = {'method': None, 'metric': None}\n",
        "\n",
        "for method in methods:\n",
        "    for metric in distance_metrics:\n",
        "        if method == 'ward' and metric != 'euclidean':\n",
        "            continue  # Ward only works with Euclidean distance\n",
        "\n",
        "        distance_matrix = pdist(train_processed, metric=metric)\n",
        "        linkage_matrix = sch.linkage(distance_matrix, method=method)\n",
        "\n",
        "        coefficient, _ = cophenet(linkage_matrix, distance_matrix)\n",
        "\n",
        "        if coefficient > best_coefficient:\n",
        "            best_coefficient = coefficient\n",
        "            best_combination = {'method': method, 'metric': metric}\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sch.dendrogram(linkage_matrix)\n",
        "        plt.title(f\"Method: {method}, Metric: {metric}, Cophenetic Coeff: {round(coefficient, 3)}\")\n",
        "        plt.xlabel(\"Samples\")\n",
        "        plt.ylabel(\"Distance\")\n",
        "        plt.show()\n",
        "\n",
        "print(\"\\nBest Combination:\")\n",
        "print(f\"Method: {best_combination['method']}\")\n",
        "print(f\"Metric: {best_combination['metric']}\")\n",
        "print(f\"Cophenetic Coefficient: {round(best_coefficient, 3)}\")\n"
      ],
      "metadata": {
        "id": "4BwlpAeakAEW"
      },
      "id": "4BwlpAeakAEW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "best_k = 0\n",
        "best_silhouette = -1\n",
        "\n",
        "for k in range(2, 11):\n",
        "    clusters = fcluster(linkage_matrix, t=k, criterion='maxclust')\n",
        "\n",
        "    silhouette_avg = silhouette_score(train_processed, clusters, metric='euclidean')\n",
        "\n",
        "    if silhouette_avg > best_silhouette:\n",
        "        best_silhouette = silhouette_avg\n",
        "        best_k = k\n",
        "\n",
        "    print(f\"Number of clusters: {k}, Silhouette Score: {round(silhouette_avg, 3)}\")\n",
        "\n",
        "print(f\"\\nBest number of clusters: {best_k} with silhouette score: {round(best_silhouette, 3)}\")\n"
      ],
      "metadata": {
        "id": "o1UbBMbnb6R9"
      },
      "id": "o1UbBMbnb6R9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "methods = ['single', 'complete', 'average', 'ward']\n",
        "distance_metrics = ['euclidean', 'cityblock']\n",
        "\n",
        "best_coefficient = -1\n",
        "best_combination = {'method': None, 'metric': None}\n",
        "\n",
        "for method in methods:\n",
        "    for metric in distance_metrics:\n",
        "        if method == 'ward' and metric != 'euclidean':\n",
        "            continue\n",
        "\n",
        "        distance_matrix = pdist(test_processed, metric=metric)\n",
        "        linkage_matrix = sch.linkage(distance_matrix, method=method)\n",
        "\n",
        "        coefficient, _ = cophenet(linkage_matrix, distance_matrix)\n",
        "\n",
        "        if coefficient > best_coefficient:\n",
        "            best_coefficient = coefficient\n",
        "            best_combination = {'method': method, 'metric': metric}\n",
        "\n",
        "distance_matrix = pdist(test_processed, metric=best_combination['metric'])\n",
        "linkage_matrix = sch.linkage(distance_matrix, method=best_combination['method'])\n",
        "\n",
        "best_silhouette_score = -1\n",
        "best_num_clusters = 0\n",
        "\n",
        "for num_clusters in range(2, 11):\n",
        "    clusters = sch.fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "\n",
        "    silhouette_avg = silhouette_score(test_processed, clusters, metric=best_combination['metric'])\n",
        "\n",
        "    if silhouette_avg > best_silhouette_score:\n",
        "        best_silhouette_score = silhouette_avg\n",
        "        best_num_clusters = num_clusters\n",
        "\n",
        "clusters = sch.fcluster(linkage_matrix, best_num_clusters, criterion='maxclust')\n",
        "\n",
        "silhouette_avg = silhouette_score(test_processed, clusters, metric=best_combination['metric'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sch.dendrogram(linkage_matrix)\n",
        "plt.title(f\"Best Dendrogram - Method: {best_combination['method']}, Metric: {best_combination['metric']}, \"\n",
        "          f\"Cophenetic Coeff: {round(best_coefficient, 3)}, Best Silhouette Score: {round(best_silhouette_score, 3)}\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nBest Combination for Test Data:\")\n",
        "print(f\"Method: {best_combination['method']}\")\n",
        "print(f\"Metric: {best_combination['metric']}\")\n",
        "print(f\"Cophenetic Coefficient: {round(best_coefficient, 3)}\")\n",
        "print(f\"Best Number of Clusters: {best_num_clusters}\")\n",
        "print(f\"Best Silhouette Score: {round(best_silhouette_score, 3)}\")\n"
      ],
      "metadata": {
        "id": "an7cORf1Oqg_"
      },
      "id": "an7cORf1Oqg_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "LB5Xr7PM1zRA",
      "metadata": {
        "id": "LB5Xr7PM1zRA"
      },
      "source": [
        "\n",
        "## Partitional Clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "silhouette_scores = []\n",
        "for k in range(2, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
        "    labels = kmeans.fit_predict(train_processed)\n",
        "    score = silhouette_score(train_processed, labels)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f'K: {k}, Silhouette Score: {score}')\n",
        "\n",
        "\n",
        "plt.plot(range(2, 10), silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Scores for different K values')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iQs-vgTE2bsg"
      },
      "id": "iQs-vgTE2bsg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kAVc5_61VV0N",
      "metadata": {
        "id": "kAVc5_61VV0N"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans.fit(train_processed)\n",
        "\n",
        "kmeans_labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.scatter(train_processed[:, 5], train_processed[:, 9], c=kmeans_labels, cmap='viridis', alpha=0.6)\n",
        "plt.scatter(centroids[:, 5], centroids[:, 9], c='red', s=200, alpha=0.8, marker='X')\n",
        "plt.title('K-means Clustering model ')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "silhouette_avg = silhouette_score(train_processed, kmeans_labels)\n",
        "print(f'Silhouette Score: {silhouette_avg}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans.fit(test_processed)\n",
        "\n",
        "kmeans_labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.scatter(test_processed[:, 5], test_processed[:, 9], c=kmeans_labels, cmap='viridis', alpha=0.6)\n",
        "plt.scatter(centroids[:, 5], centroids[:, 9], c='red', s=200, alpha=0.8, marker='X')\n",
        "plt.title('Test data prediction for K-means clustering')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "silhouette_avg = silhouette_score(test_processed, kmeans_labels)\n",
        "print(f'Silhouette Score: {silhouette_avg}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YYCkJT2WWAdo"
      },
      "id": "YYCkJT2WWAdo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 20)\n",
        "train_pca = pca.fit_transform(train_processed)\n"
      ],
      "metadata": {
        "id": "MS2C7bqiJ57L"
      },
      "id": "MS2C7bqiJ57L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans.fit(train_pca)\n",
        "\n",
        "kmeans_labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(train_pca[:, 0], train_pca[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.8, marker='X')\n",
        "plt.title('K-means Clustering model')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "silhouette_avg = silhouette_score(train_pca, kmeans_labels)\n",
        "print(f'Silhouette Score: {silhouette_avg}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7LP_6YOSKzGQ"
      },
      "id": "7LP_6YOSKzGQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN"
      ],
      "metadata": {
        "id": "1SxrNr3zqLPq"
      },
      "id": "1SxrNr3zqLPq"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "eps_values = np.arange(0.5, 2.5, 5)\n",
        "min_samples_values = [3, 5, 10, 15, 20]\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(train_processed)\n",
        "\n",
        "        unique_labels = set(labels)\n",
        "        if len(unique_labels) > 1:\n",
        "            score = silhouette_score(train_processed, labels)\n",
        "            print(f\"eps: {eps:.2f}, min_samples: {min_samples}, Silhouette Score: {score:.4f}\")\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "\n",
        "if best_params:\n",
        "    print(\"\\nBest Parameters:\", best_params)\n",
        "    print(\"Best Silhouette Score:\", best_score)\n",
        "\n",
        "    best_dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "    best_labels = best_dbscan.fit_predict(train_processed)\n",
        "else:\n",
        "    print(\"No valid clusters found during grid search.\")\n",
        "    best_labels = np.zeros(train_processed.shape[0], dtype=int)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(train_processed[:, 5], train_processed[:, 9], c=best_labels, cmap='viridis', alpha=0.6)\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "43VfuB5zv8nQ"
      },
      "id": "43VfuB5zv8nQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = train_pca\n",
        "\n",
        "dbscan = DBSCAN(eps=15, min_samples=100)\n",
        "labels = dbscan.fit_predict(data)\n",
        "\n",
        "eps_values = np.arange(0.5, 2.5, 5)\n",
        "min_samples_values = [3, 5, 10, 15, 20]\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(data)\n",
        "\n",
        "        unique_labels = set(labels)\n",
        "        if len(unique_labels) > 1:\n",
        "            score = silhouette_score(data, labels)\n",
        "            print(f\"eps: {eps:.2f}, min_samples: {min_samples}, Silhouette Score: {score:.4f}\")\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "\n",
        "if best_params:\n",
        "    print(\"\\nBest Parameters:\", best_params)\n",
        "    print(\"Best Silhouette Score:\", best_score)\n",
        "\n",
        "    best_dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "    best_labels = best_dbscan.fit_predict(data)\n",
        "else:\n",
        "    print(\"No valid clusters found during grid search.\")\n",
        "    best_labels = np.zeros(data.shape[0], dtype=int)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data[:, 0], data[:, 1], c=best_labels, cmap='viridis', alpha=0.6)\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pq8UMrE3MnDt"
      },
      "id": "pq8UMrE3MnDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Mixture Model\n",
        "\n"
      ],
      "metadata": {
        "id": "egA49oXwqQDL"
      },
      "id": "egA49oXwqQDL"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "    'n_components': [2, 3, 4, 5, 6, 7],\n",
        "    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n",
        "}\n",
        "\n",
        "best_bic = np.inf\n",
        "best_params = None\n",
        "best_gmm = None\n",
        "best_labels = None\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "\n",
        "    gmm = GaussianMixture(**params, random_state=42)\n",
        "    gmm.fit(train_processed)\n",
        "    bic = gmm.bic(train_processed)\n",
        "\n",
        "    labels = gmm.predict(train_processed)\n",
        "\n",
        "    if bic < best_bic:\n",
        "        best_bic = bic\n",
        "        best_params = params\n",
        "        best_gmm = gmm\n",
        "        best_labels = labels\n",
        "\n",
        "    print(f\"n_components: {params['n_components']}, covariance_type: {params['covariance_type']}, BIC: {bic}\")\n",
        "\n",
        "print(f\"\\nBest Parameters: {best_params}\")\n",
        "print(f\"Best BIC: {best_bic}\")\n",
        "\n",
        "\n",
        "best_labels = best_gmm.predict(train_processed)\n",
        "means = best_gmm.means_\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(train_processed[:, 5],train_processed[:, 9], c=best_labels, cmap='viridis', alpha=0.6)\n",
        "plt.title(f'GMM Clustering with {best_params[\"n_components\"]} Components')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "po96Huce0NFi"
      },
      "id": "po96Huce0NFi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = best_gmm.predict(test_processed)\n",
        "bic_test = best_gmm.bic(test_processed)\n",
        "print(f\"BIC for Test Data: {bic_test}\")\n",
        "silhouette_test = silhouette_score(test_processed, test_labels)\n",
        "print(f\"Silhouette Score for Test Data: {silhouette_test}\")\n",
        "print(f\"\\nBest Parameters: {best_params}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.scatter(test_processed[:, 5], test_processed[:, 9], c=test_labels, cmap='viridis', alpha=0.6)\n",
        "plt.title(f'GMM Clustering with {best_params[\"n_components\"]} Components')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dwn3VyXSqTDM"
      },
      "id": "Dwn3VyXSqTDM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n",
        "}\n",
        "\n",
        "best_bic = np.inf\n",
        "best_params = None\n",
        "best_gmm = None\n",
        "best_labels = None\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    gmm = GaussianMixture(**params, random_state=42)\n",
        "    gmm.fit(train_pca)\n",
        "    bic = gmm.bic(train_pca)\n",
        "\n",
        "    if bic < best_bic:\n",
        "        best_bic = bic\n",
        "        best_params = params\n",
        "        best_gmm = gmm\n",
        "        best_labels = gmm.predict(train_pca)\n",
        "\n",
        "print(f\"\\nBest Parameters: {best_params}\")\n",
        "print(f\"Best BIC: {best_bic}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(train_pca[:, 0], train_pca[:, 1], c=best_labels, cmap='viridis', alpha=0.6)\n",
        "plt.title(f'GMM Clustering with {best_params[\"n_components\"]} Components')\n",
        "plt.xlabel('PCA Feature 1')\n",
        "plt.ylabel('PCA Feature 2')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G40hmYk5yFRP"
      },
      "id": "G40hmYk5yFRP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}